{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost final model"
      ],
      "metadata": {
        "id": "9lNBySWJYjRw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_fwIHdAVQRB4"
      },
      "outputs": [],
      "source": [
        "''' !pip install -q gdown\n",
        "! pip install -q catboost\n",
        "! pip install -q category_encoders '''\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "import re\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.sparse import hstack, vstack, csr_matrix\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "2Hpx54htYoE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = \"1lvt_himfQapYiUPbaS07dONMZ718cfk0\"\n",
        "gdown.download(id=file_id, output=\"Mercari-dataset.tsv\", quiet=False)\n",
        "\n",
        "df = pd.read_csv(\"Mercari-dataset.tsv\", sep=\"\\t\")\n",
        "''' df.head() '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "2uI708h3QYwa",
        "outputId": "7a2c8310-c063-4994-a0ea-b8520b64d059"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1lvt_himfQapYiUPbaS07dONMZ718cfk0\n",
            "From (redirected): https://drive.google.com/uc?id=1lvt_himfQapYiUPbaS07dONMZ718cfk0&confirm=t&uuid=ad7124a7-5f6b-4afa-b9cb-7d159da7a8fb\n",
            "To: /content/Mercari-dataset.tsv\n",
            "100%|██████████| 338M/338M [00:02<00:00, 117MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' df.head() '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "chkguyvrYpg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "df['log_price'] = np.log1p(df['price'])\n",
        "\n",
        "X = df.drop(columns=['price', 'log_price', 'train_id'])\n",
        "y = df['log_price']\n",
        "\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "IaOUWhCnQcNC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shipping feature\n",
        "After testing a couple stratedies mode gave the best results"
      ],
      "metadata": {
        "id": "3hzx9QuYYtw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShippingToInt64(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        if self.column in X.columns:\n",
        "            X[self.column] = pd.to_numeric(X[self.column], errors='coerce').astype('Int64')\n",
        "        return X\n",
        "\n",
        "class FillWithMode(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "        self.mode = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.mode = X[self.column].mode(dropna=True)[0]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = X[self.column].fillna(self.mode)\n",
        "        return X[[self.column]]\n",
        "\n",
        "shipping_pipeline = Pipeline([\n",
        "    ('coerce_Int64', ShippingToInt64(column='shipping')),\n",
        "    ('fill_mode', FillWithMode(column='shipping')),\n",
        "])"
      ],
      "metadata": {
        "id": "cwKaHROMQeEP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Item condition feature\n",
        "Created missingness indicator"
      ],
      "metadata": {
        "id": "WC9BVI5QafYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoerceToInt64(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = X[self.column].astype(\"Int64\")\n",
        "        return X[[self.column]]\n",
        "\n",
        "class FillNAInt64(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, fill_value=-1):\n",
        "        self.column = column\n",
        "        self.fill_value = fill_value\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = X[self.column].fillna(self.fill_value)\n",
        "        return X[[self.column]]\n",
        "\n",
        "condition_pipeline = Pipeline([\n",
        "    ('coerce_Int64', CoerceToInt64(column='item_condition_id')),\n",
        "    ('fill_Int64', FillNAInt64(column='item_condition_id')),\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "CKzS4XxGX7s7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brand name\n",
        "Target encoding for high cardinality"
      ],
      "metadata": {
        "id": "lSixrIY5XyAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalizeTextColumn(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, lower=True, fill_value='missing'):\n",
        "        self.column = column\n",
        "        self.lower = lower\n",
        "        self.fill_value = fill_value\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        col = X.columns[0] if len(X.columns) == 1 else self.column\n",
        "\n",
        "        X[col] = X[col].astype(str)\n",
        "        X[col] = X[col].str.strip()\n",
        "        X[col] = X[col].replace(r'^\\s*$', np.nan, regex=True)\n",
        "        X[col] = X[col].fillna(self.fill_value)\n",
        "\n",
        "        if self.lower:\n",
        "            X[col] = X[col].str.lower()\n",
        "\n",
        "        return pd.DataFrame({self.column: X[col]})\n",
        "\n",
        "class SafeTargetEncoderColumn(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, fill_value='missing', smoothing=1.0):\n",
        "        self.column = column\n",
        "        self.fill_value = fill_value\n",
        "        self.smoothing = smoothing\n",
        "        self.encoder = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = X.copy()\n",
        "        col = X.columns[0] if len(X.columns) == 1 else self.column\n",
        "\n",
        "        X[col] = X[col].fillna(self.fill_value).astype(str)\n",
        "        self.encoder = TargetEncoder(\n",
        "            cols=[col],\n",
        "            smoothing=self.smoothing,\n",
        "            handle_missing='value',\n",
        "            handle_unknown='value'\n",
        "        )\n",
        "        self.encoder.fit(X[[col]], y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        col = X.columns[0] if len(X.columns) == 1 else self.column\n",
        "\n",
        "        X[col] = X[col].fillna(self.fill_value).astype(str)\n",
        "        encoded = self.encoder.transform(X[[col]])\n",
        "        return encoded\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "brand_pipeline = Pipeline([\n",
        "    ('normalize', NormalizeTextColumn(column='brand_name')),\n",
        "    ('label_encode', SafeTargetEncoderColumn(column='brand_name'))\n",
        "])"
      ],
      "metadata": {
        "id": "hk9_s7nLXzhB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Category name feature\n",
        "Tree target encoding to preserve hierarchy, dynamically batch to minimise ram usage"
      ],
      "metadata": {
        "id": "plUmuSM7aRyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalizeCategoryName(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column='category_name', lower=True):\n",
        "        self.column = column\n",
        "        self.lower = lower\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        col = X.columns[0]\n",
        "\n",
        "        X[col] = X[col].astype(str)\n",
        "        X[col] = X[col].str.strip()\n",
        "        X[col] = X[col].replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "        if self.lower:\n",
        "            X[col] = X[col].str.lower()\n",
        "\n",
        "        X[col] = X[col].str.replace(r'/+', '/', regex=True)\n",
        "        X[col] = X[col].str.strip('/')\n",
        "        X[col] = X[col].str.replace(r'[^a-z0-9/ &+-]', '', regex=True)\n",
        "\n",
        "        return pd.DataFrame({self.column: X[col]})\n",
        "\n",
        "class SafeCategorySplitter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, source_column='category_name', fill_value='missing'):\n",
        "        self.source_column = source_column\n",
        "        self.fill_value = fill_value\n",
        "        self.output_columns = ['cat_lvl_1', 'cat_lvl_2', 'cat_lvl_3']\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        default_fill = f\"{self.fill_value}/{self.fill_value}/{self.fill_value}\"\n",
        "        X[self.source_column] = X[self.source_column].fillna(default_fill)\n",
        "\n",
        "        splits = X[self.source_column].str.split('/', n=2, expand=True)\n",
        "\n",
        "        for i in range(3):\n",
        "            if i not in splits.columns:\n",
        "                splits[i] = self.fill_value\n",
        "\n",
        "        splits.columns = self.output_columns\n",
        "\n",
        "        splits = splits.fillna(self.fill_value)\n",
        "\n",
        "        return splits\n",
        "\n",
        "\n",
        "\n",
        "class TreeBasedTargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 col_lvl_1='cat_lvl_1',\n",
        "                 col_lvl_2='cat_lvl_2',\n",
        "                 col_lvl_3='cat_lvl_3',\n",
        "                 smoothing=5,\n",
        "                 n_splits=5,\n",
        "                 random_state=42):\n",
        "        self.col_lvl_1 = col_lvl_1\n",
        "        self.col_lvl_2 = col_lvl_2\n",
        "        self.col_lvl_3 = col_lvl_3\n",
        "        self.smoothing = smoothing\n",
        "        self.n_splits = n_splits\n",
        "        self.random_state = random_state\n",
        "        self.encoding_maps_ = {}\n",
        "        self.global_mean_ = None\n",
        "\n",
        "    def _combine_levels(self, X):\n",
        "        X = X.copy()\n",
        "        X['cat1_2'] = X[self.col_lvl_1] + '/' + X[self.col_lvl_2]\n",
        "        X['cat1_2_3'] = X[self.col_lvl_1] + '/' + X[self.col_lvl_2] + '/' + X[self.col_lvl_3]\n",
        "        return X\n",
        "\n",
        "    def _fit_target_encoding(self, series, y):\n",
        "        series = series.astype(str)\n",
        "        mean = y.mean()\n",
        "        stats = y.groupby(series).agg(['mean', 'count'])\n",
        "        smooth = (stats['mean'] * stats['count'] + mean * self.smoothing) / (stats['count'] + self.smoothing)\n",
        "        return smooth.to_dict()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = X.copy()\n",
        "        y = pd.Series(y, index=X.index)\n",
        "        X = self._combine_levels(X)\n",
        "\n",
        "        self.global_mean_ = y.mean()\n",
        "        self.encoding_maps_ = {}\n",
        "\n",
        "        for col in [self.col_lvl_1, 'cat1_2', 'cat1_2_3']:\n",
        "            oof_encoded = pd.Series(np.nan, index=X.index, dtype=float)\n",
        "            kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
        "\n",
        "            for train_idx, val_idx in kf.split(X):\n",
        "                X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "                X_val = X.iloc[val_idx]\n",
        "                enc_map = self._fit_target_encoding(X_train[col], y_train)\n",
        "                oof_encoded.iloc[val_idx] = X_val[col].astype(str).map(enc_map)\n",
        "\n",
        "            oof_encoded = oof_encoded.fillna(self.global_mean_)\n",
        "            X[f'{col}_enc'] = oof_encoded\n",
        "\n",
        "            self.encoding_maps_[col] = self._fit_target_encoding(X[col], y)\n",
        "\n",
        "        self.fitted_columns_ = [f'{col}_enc' for col in [self.col_lvl_1, 'cat1_2', 'cat1_2_3']]\n",
        "        self.fitted_data_ = X[self.fitted_columns_].copy()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = self._combine_levels(X)\n",
        "        X_encoded = pd.DataFrame(index=X.index)\n",
        "\n",
        "        for col in [self.col_lvl_1, 'cat1_2', 'cat1_2_3']:\n",
        "            encoded = X[col].astype(str).map(self.encoding_maps_[col])\n",
        "            X_encoded[f'{col}_enc'] = encoded.fillna(self.global_mean_).astype(float)\n",
        "\n",
        "        return X_encoded\n",
        "\n",
        "\n",
        "category_pipeline = Pipeline([\n",
        "    ('normalize', NormalizeCategoryName(column='category_name')),\n",
        "    ('split', SafeCategorySplitter(source_column='category_name')),\n",
        "    ('target_encode', TreeBasedTargetEncoder(\n",
        "        col_lvl_1='cat_lvl_1',\n",
        "        col_lvl_2='cat_lvl_2',\n",
        "        col_lvl_3='cat_lvl_3',\n",
        "        smoothing=5,\n",
        "        n_splits=5,\n",
        "        random_state=42\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "Nzr-AVULX-lw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name & item description feature\n",
        "Tokenization remains the same just added meta tags"
      ],
      "metadata": {
        "id": "tR1fCntpaAjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TFIDFVectorizerWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, max_features=1000, stop_words=None, ngram_range=(1, 1)):\n",
        "        self.column = column\n",
        "        self.max_features = max_features\n",
        "        self.stop_words = stop_words\n",
        "        self.ngram_range = ngram_range\n",
        "        self.vectorizer = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        col = X.columns[0] if len(X.columns) == 1 else self.column\n",
        "        X_col = X[col].fillna('').astype(str)\n",
        "\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=self.max_features,\n",
        "            stop_words=self.stop_words,\n",
        "            ngram_range=self.ngram_range\n",
        "        )\n",
        "        self.vectorizer.fit(X_col)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        col = X.columns[0] if len(X.columns) == 1 else self.column\n",
        "        X_col = X[col].fillna('').astype(str)\n",
        "        return self.vectorizer.transform(X_col)\n",
        "\n",
        "\n",
        "name_pipeline = Pipeline([\n",
        "    ('normalize', NormalizeTextColumn(column='name')),\n",
        "    ('vectorize', TFIDFVectorizerWrapper(\n",
        "        column='name',\n",
        "        max_features=400,\n",
        "        stop_words=None,\n",
        "        ngram_range=(1, 2)\n",
        "    ))\n",
        "])\n",
        "\n",
        "item_pipeline = Pipeline([\n",
        "    ('normalize', NormalizeTextColumn(column='item_description')),\n",
        "    ('vectorize', TFIDFVectorizerWrapper(\n",
        "        column='item_description',\n",
        "        max_features=1500,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2)\n",
        "    ))\n",
        "])\n"
      ],
      "metadata": {
        "id": "Hs5a0irQYENX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preproccessor"
      ],
      "metadata": {
        "id": "YURZV0GBZ9Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer([\n",
        "    ('shipping', shipping_pipeline, ['shipping']),\n",
        "    ('item_condition', condition_pipeline, ['item_condition_id']),\n",
        "    ('brand', brand_pipeline, ['brand_name']),\n",
        "    ('category', category_pipeline, ['category_name']),\n",
        "    ('name_text', name_pipeline, ['name']),\n",
        "    ('description_text', item_pipeline, ['item_description']),\n",
        "])"
      ],
      "metadata": {
        "id": "xi26DWC_YGQ-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "5-PwhcjiYOrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('regressor', xgb_model)\n",
        "])\n",
        "\n",
        "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "pipeline.fit(X_subtrain, y_subtrain)\n",
        "y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "r2_val = r2_score(y_val, y_val_pred)\n",
        "mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "rmse_val = np.sqrt(mse_val)\n",
        "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_test_pred = pipeline.predict(X_holdout)\n",
        "\n",
        "r2_test = r2_score(y_holdout, y_test_pred)\n",
        "mse_test = mean_squared_error(y_holdout, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "mae_test = mean_absolute_error(y_holdout, y_test_pred)\n",
        "\n",
        "print(\"XGBoost\")\n",
        "print(\"R2:\", r2_val)\n",
        "print(\"MSE:\", mse_val)\n",
        "print(\"RMSE:\", rmse_val)\n",
        "print(\"MAE:\", mae_val)\n",
        "print(\"Hold-Out Test Metrics:\")\n",
        "print(\"R2\", r2_test)\n",
        "print(\"MSE:\", mse_test)\n",
        "print(\"RMSE:\", rmse_test)\n",
        "print(\"MAE:\", mae_test)"
      ],
      "metadata": {
        "id": "BSj6-fHPYOKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f752886f-d5e0-4b08-c710-d18910f5c7a7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost\n",
            "R2: 0.5216645063169298\n",
            "MSE: 0.26847900127943697\n",
            "RMSE: 0.518149593533988\n",
            "MAE: 0.3901273426714204\n",
            "Hold-Out Test Metrics:\n",
            "R2 0.5250248962260882\n",
            "MSE: 0.2679365567139123\n",
            "RMSE: 0.5176258848955607\n",
            "MAE: 0.389446412177696\n"
          ]
        }
      ]
    }
  ]
}