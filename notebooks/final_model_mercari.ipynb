{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost final model"
      ],
      "metadata": {
        "id": "9lNBySWJYjRw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "_fwIHdAVQRB4"
      },
      "outputs": [],
      "source": [
        "''' !pip install -q gdown\n",
        "! pip install -q category_encoders\n",
        "! pip install -q optuna '''\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "import re\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.sparse import hstack, vstack, csr_matrix\n",
        "import psutil\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "2Hpx54htYoE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = \"1lvt_himfQapYiUPbaS07dONMZ718cfk0\"\n",
        "gdown.download(id=file_id, output=\"Mercari-dataset.tsv\", quiet=False)\n",
        "\n",
        "df = pd.read_csv(\"Mercari-dataset.tsv\", sep=\"\\t\")\n",
        "''' df.head() '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "2uI708h3QYwa",
        "outputId": "91d703cc-1ffc-41e2-de55-a5d4bf471c52"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1lvt_himfQapYiUPbaS07dONMZ718cfk0\n",
            "From (redirected): https://drive.google.com/uc?id=1lvt_himfQapYiUPbaS07dONMZ718cfk0&confirm=t&uuid=994cfaa1-9d74-429b-b167-713b3e9c6aaf\n",
            "To: /content/Mercari-dataset.tsv\n",
            "100%|██████████| 338M/338M [00:03<00:00, 89.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' df.head() '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "clipping outliers in target improved error metrics"
      ],
      "metadata": {
        "id": "chkguyvrYpg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "df['log_price'] = np.log1p(df['price'])\n",
        "\n",
        "X = df.drop(columns=['price', 'log_price', 'train_id'])\n",
        "y = df['log_price']\n",
        "\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "IaOUWhCnQcNC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shipping feature\n",
        "After testing a couple stratedies mode gave the best results"
      ],
      "metadata": {
        "id": "3hzx9QuYYtw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShippingToInt64(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        if self.column in X.columns:\n",
        "            X[self.column] = pd.to_numeric(X[self.column], errors='coerce').astype('Int64')\n",
        "        return X\n",
        "\n",
        "class FillWithMode(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "        self.mode = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.mode = X[self.column].mode(dropna=True)[0]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = X[self.column].fillna(self.mode)\n",
        "        return X[[self.column]]\n",
        "\n",
        "shipping_pipeline = Pipeline([\n",
        "    ('coerce_Int64', ShippingToInt64(column='shipping')),\n",
        "    ('fill_mode', FillWithMode(column='shipping')),\n",
        "])"
      ],
      "metadata": {
        "id": "cwKaHROMQeEP"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Item condition feature\n",
        "Created missingness indicator"
      ],
      "metadata": {
        "id": "WC9BVI5QafYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoerceToInt64(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = X[self.column].astype(\"Int64\")\n",
        "        return X[[self.column]]\n",
        "\n",
        "class FillNAInt64(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, fill_value=-1):\n",
        "        self.column = column\n",
        "        self.fill_value = fill_value\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = X[self.column].fillna(self.fill_value)\n",
        "        return X[[self.column]]\n",
        "\n",
        "condition_pipeline = Pipeline([\n",
        "    ('coerce_Int64', CoerceToInt64(column='item_condition_id')),\n",
        "    ('fill_Int64', FillNAInt64(column='item_condition_id')),\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "CKzS4XxGX7s7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brand name\n",
        "Target encoding for high cardinality"
      ],
      "metadata": {
        "id": "lSixrIY5XyAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalizeTextColumn(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, lower=True, fill_value='missing'):\n",
        "        self.column = column\n",
        "        self.lower = lower\n",
        "        self.fill_value = fill_value\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        col = X.columns[0] if len(X.columns) == 1 else self.column\n",
        "\n",
        "        X[col] = X[col].astype(str)\n",
        "        X[col] = X[col].str.strip()\n",
        "        X[col] = X[col].replace(r'^\\s*$', np.nan, regex=True)\n",
        "        X[col] = X[col].fillna(self.fill_value)\n",
        "\n",
        "        if self.lower:\n",
        "            X[col] = X[col].str.lower()\n",
        "\n",
        "        return X\n",
        "\n",
        "class SafeTargetEncoderColumn(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, fill_value='missing', smoothing=1.0):\n",
        "        self.column = column\n",
        "        self.fill_value = fill_value\n",
        "        self.smoothing = smoothing\n",
        "        self.encoder = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = X.copy()\n",
        "        col = self.column\n",
        "\n",
        "        X[col] = X[col].fillna(self.fill_value).astype(str)\n",
        "        self.encoder = TargetEncoder(\n",
        "            cols=[col],\n",
        "            smoothing=self.smoothing,\n",
        "            handle_missing='value',\n",
        "            handle_unknown='value'\n",
        "        )\n",
        "        self.encoder.fit(X[[col]], y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        col = self.column\n",
        "\n",
        "        X[col] = X[col].fillna(self.fill_value).astype(str)\n",
        "        X[col] = self.encoder.transform(X[[col]])[col]\n",
        "        return X\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "class MissingIndicator(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, output_column=None):\n",
        "        self.column = column\n",
        "        self.output_column = output_column or f'{column}_missing_flag'\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.output_column] = X[self.column].isna().astype(int)\n",
        "        return X\n",
        "\n",
        "brand_pipeline = Pipeline([\n",
        "    ('missing_flag', MissingIndicator(column='brand_name')),\n",
        "    ('normalize', NormalizeTextColumn(column='brand_name')),\n",
        "    ('label_encode', SafeTargetEncoderColumn(column='brand_name'))\n",
        "])"
      ],
      "metadata": {
        "id": "hk9_s7nLXzhB"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Category name feature\n",
        "Tree target encoding to preserve hierarchy, dynamically batch to minimise ram usage"
      ],
      "metadata": {
        "id": "plUmuSM7aRyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalizeCategoryName(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column='category_name', lower=True):\n",
        "        self.column = column\n",
        "        self.lower = lower\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        col = X.columns[0]\n",
        "\n",
        "        X[col] = X[col].astype(str)\n",
        "        X[col] = X[col].str.strip()\n",
        "        X[col] = X[col].replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "        if self.lower:\n",
        "            X[col] = X[col].str.lower()\n",
        "\n",
        "        X[col] = X[col].str.replace(r'/+', '/', regex=True)\n",
        "        X[col] = X[col].str.strip('/')\n",
        "        X[col] = X[col].str.replace(r'[^a-z0-9/ &+-]', '', regex=True)\n",
        "\n",
        "        return pd.DataFrame({self.column: X[col]})\n",
        "\n",
        "class SafeCategorySplitter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, source_column='category_name', fill_value='missing'):\n",
        "        self.source_column = source_column\n",
        "        self.fill_value = fill_value\n",
        "        self.output_columns = ['cat_lvl_1', 'cat_lvl_2', 'cat_lvl_3']\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        default_fill = f\"{self.fill_value}/{self.fill_value}/{self.fill_value}\"\n",
        "        X[self.source_column] = X[self.source_column].fillna(default_fill)\n",
        "\n",
        "        splits = X[self.source_column].str.split('/', n=2, expand=True)\n",
        "\n",
        "        for i in range(3):\n",
        "            if i not in splits.columns:\n",
        "                splits[i] = self.fill_value\n",
        "\n",
        "        splits.columns = self.output_columns\n",
        "\n",
        "        splits = splits.fillna(self.fill_value)\n",
        "\n",
        "        return splits\n",
        "\n",
        "\n",
        "\n",
        "class TreeBasedTargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 col_lvl_1='cat_lvl_1',\n",
        "                 col_lvl_2='cat_lvl_2',\n",
        "                 col_lvl_3='cat_lvl_3',\n",
        "                 smoothing=5,\n",
        "                 n_splits=5,\n",
        "                 random_state=42):\n",
        "        self.col_lvl_1 = col_lvl_1\n",
        "        self.col_lvl_2 = col_lvl_2\n",
        "        self.col_lvl_3 = col_lvl_3\n",
        "        self.smoothing = smoothing\n",
        "        self.n_splits = n_splits\n",
        "        self.random_state = random_state\n",
        "        self.encoding_maps_ = {}\n",
        "        self.global_mean_ = None\n",
        "\n",
        "    def _combine_levels(self, X):\n",
        "        X = X.copy()\n",
        "        X['cat1_2'] = X[self.col_lvl_1] + '/' + X[self.col_lvl_2]\n",
        "        X['cat1_2_3'] = X[self.col_lvl_1] + '/' + X[self.col_lvl_2] + '/' + X[self.col_lvl_3]\n",
        "        return X\n",
        "\n",
        "    def _fit_target_encoding(self, series, y):\n",
        "        series = series.astype(str)\n",
        "        mean = y.mean()\n",
        "        stats = y.groupby(series).agg(['mean', 'count'])\n",
        "        smooth = (stats['mean'] * stats['count'] + mean * self.smoothing) / (stats['count'] + self.smoothing)\n",
        "        return smooth.to_dict()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = X.copy()\n",
        "        y = pd.Series(y, index=X.index)\n",
        "        X = self._combine_levels(X)\n",
        "\n",
        "        self.global_mean_ = y.mean()\n",
        "        self.encoding_maps_ = {}\n",
        "\n",
        "        for col in [self.col_lvl_1, 'cat1_2', 'cat1_2_3']:\n",
        "            oof_encoded = pd.Series(np.nan, index=X.index, dtype=float)\n",
        "            kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
        "\n",
        "            for train_idx, val_idx in kf.split(X):\n",
        "                X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "                X_val = X.iloc[val_idx]\n",
        "                enc_map = self._fit_target_encoding(X_train[col], y_train)\n",
        "                oof_encoded.iloc[val_idx] = X_val[col].astype(str).map(enc_map)\n",
        "\n",
        "            oof_encoded = oof_encoded.fillna(self.global_mean_)\n",
        "            X[f'{col}_enc'] = oof_encoded\n",
        "\n",
        "            self.encoding_maps_[col] = self._fit_target_encoding(X[col], y)\n",
        "\n",
        "        self.fitted_columns_ = [f'{col}_enc' for col in [self.col_lvl_1, 'cat1_2', 'cat1_2_3']]\n",
        "        self.fitted_data_ = X[self.fitted_columns_].copy()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = self._combine_levels(X)\n",
        "        X_encoded = pd.DataFrame(index=X.index)\n",
        "\n",
        "        for col in [self.col_lvl_1, 'cat1_2', 'cat1_2_3']:\n",
        "            encoded = X[col].astype(str).map(self.encoding_maps_[col])\n",
        "            X_encoded[f'{col}_enc'] = encoded.fillna(self.global_mean_).astype(float)\n",
        "\n",
        "        return X_encoded\n",
        "\n",
        "\n",
        "category_pipeline = Pipeline([\n",
        "    ('normalize', NormalizeCategoryName(column='category_name')),\n",
        "    ('split', SafeCategorySplitter(source_column='category_name')),\n",
        "    ('target_encode', TreeBasedTargetEncoder(\n",
        "        col_lvl_1='cat_lvl_1',\n",
        "        col_lvl_2='cat_lvl_2',\n",
        "        col_lvl_3='cat_lvl_3',\n",
        "        smoothing=5,\n",
        "        n_splits=5,\n",
        "        random_state=42\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "Nzr-AVULX-lw"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name & item description feature\n",
        "Tokenization remains the same simply expirimented with tags, range and max features"
      ],
      "metadata": {
        "id": "tR1fCntpaAjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDFVectorizerWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, max_features=1000, stop_words=None, ngram_range=(1, 1)):\n",
        "        self.column = column\n",
        "        self.max_features = max_features\n",
        "        self.stop_words = stop_words\n",
        "        self.ngram_range = ngram_range\n",
        "        self.vectorizer = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        col = self.column\n",
        "        X_col = X[col].fillna('').astype(str)\n",
        "\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=self.max_features,\n",
        "            stop_words=self.stop_words,\n",
        "            ngram_range=self.ngram_range\n",
        "        )\n",
        "        self.vectorizer.fit(X_col)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        col = self.column\n",
        "        X_col = X[col].fillna('').astype(str)\n",
        "\n",
        "        tfidf_matrix = self.vectorizer.transform(X_col)\n",
        "\n",
        "        word_count = X_col.apply(lambda x: len(x.split())).values.reshape(-1, 1)\n",
        "        word_count_sparse = csr_matrix(word_count)\n",
        "\n",
        "        nnz = (tfidf_matrix != 0).sum(axis=1).A1.reshape(-1, 1)\n",
        "        nnz_sparse = csr_matrix(nnz)\n",
        "\n",
        "        if col == 'name':\n",
        "            common_keywords = ['new', 'used', 'bundle']\n",
        "            name_common_word_flag = X_col.apply(\n",
        "                lambda text: int(any(word in text.lower() for word in common_keywords))\n",
        "            ).values.reshape(-1, 1)\n",
        "            name_common_word_sparse = csr_matrix(name_common_word_flag)\n",
        "\n",
        "            max_tfidf = tfidf_matrix.max(axis=1).toarray().reshape(-1, 1)\n",
        "            max_tfidf_sparse = csr_matrix(max_tfidf)\n",
        "\n",
        "            return hstack([\n",
        "                tfidf_matrix,\n",
        "                word_count_sparse,\n",
        "                max_tfidf_sparse,\n",
        "                name_common_word_sparse\n",
        "            ])\n",
        "\n",
        "        elif col == 'item_description':\n",
        "            mean_tfidf = tfidf_matrix.sum(axis=1).A1.reshape(-1, 1) / (nnz + 1e-6)\n",
        "            mean_tfidf_sparse = csr_matrix(mean_tfidf)\n",
        "\n",
        "            return hstack([\n",
        "                tfidf_matrix,\n",
        "                word_count_sparse,\n",
        "                nnz_sparse,\n",
        "                mean_tfidf_sparse\n",
        "            ])\n",
        "\n",
        "        return hstack([tfidf_matrix, word_count_sparse])\n",
        "\n",
        "name_pipeline = Pipeline([\n",
        "    ('normalize', NormalizeTextColumn(column='name')),\n",
        "    ('vectorize', TFIDFVectorizerWrapper(\n",
        "        column='name',\n",
        "        max_features=400,\n",
        "        stop_words=None,\n",
        "        ngram_range=(1, 2)\n",
        "    ))\n",
        "])\n",
        "\n",
        "item_pipeline = Pipeline([\n",
        "    ('normalize', NormalizeTextColumn(column='item_description')),\n",
        "    ('vectorize', TFIDFVectorizerWrapper(\n",
        "        column='item_description',\n",
        "        max_features=1500,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2)\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "Hs5a0irQYENX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preproccessor"
      ],
      "metadata": {
        "id": "YURZV0GBZ9Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer([\n",
        "    ('shipping', shipping_pipeline, ['shipping']),\n",
        "    ('item_condition', condition_pipeline, ['item_condition_id']),\n",
        "    ('brand', brand_pipeline, ['brand_name']),\n",
        "    ('category', category_pipeline, ['category_name']),\n",
        "    ('name_text', name_pipeline, ['name']),\n",
        "    ('description_text', item_pipeline, ['item_description']),\n",
        "])"
      ],
      "metadata": {
        "id": "xi26DWC_YGQ-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "O2eRTRXNwkoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.28876729035886295,\n",
        "    subsample=0.6249548549636976,\n",
        "    colsample_bytree=0.5162030917981775,\n",
        "    reg_alpha=1.623450951405694,\n",
        "    reg_lambda=4.2760846903011895,\n",
        "    random_state=42,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('regressor', xgb_model)\n",
        "])\n",
        "\n",
        "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "pipeline.fit(X_subtrain, y_subtrain)\n",
        "y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "r2_val = r2_score(y_val, y_val_pred)\n",
        "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_test_pred = pipeline.predict(X_holdout)\n",
        "\n",
        "r2_test = r2_score(y_holdout, y_test_pred)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_holdout, y_test_pred))\n",
        "mae_test = mean_absolute_error(y_holdout, y_test_pred)\n",
        "\n",
        "print(\"XGBoost\")\n",
        "print(\"R2:\", r2_val)\n",
        "print(\"RMSE:\", rmse_val)\n",
        "print(\"MAE:\", mae_val)\n",
        "print(\"Hold-Out Test Metrics:\")\n",
        "print(\"R2:\", r2_test)\n",
        "print(\"RMSE:\", rmse_test)\n",
        "print(\"MAE:\", mae_test)\n"
      ],
      "metadata": {
        "id": "BSj6-fHPYOKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2322bd2f-3570-4dd5-9dde-77cbea3df4d3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost\n",
            "R2: 0.5513238354153979\n",
            "RMSE: 0.5018285404550334\n",
            "MAE: 0.3762923001617534\n",
            "Hold-Out Test Metrics:\n",
            "R2: 0.5528617939627936\n",
            "RMSE: 0.502228614257657\n",
            "MAE: 0.37595349203995004\n"
          ]
        }
      ]
    }
  ]
}